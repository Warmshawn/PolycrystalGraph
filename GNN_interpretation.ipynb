{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "RWSJpsyKqHjH",
    "outputId": "f29fd9d7-3238-4d92-fc3c-ac65dfee8129"
   },
   "outputs": [],
   "source": [
    "# Mount the google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "g7wSA6q-M7hT",
    "outputId": "ac4dd3a4-4f48-465d-9727-0dc4dbef0a97"
   },
   "outputs": [],
   "source": [
    "# go to the folder contains subfolder data/\n",
    "%cd /content/drive/My\\ Drive/microstructure/GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XdxhRalNTRs"
   },
   "outputs": [],
   "source": [
    "# load the data into the dataset\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import sparse\n",
    "\n",
    "class GraphDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        max_node = 300   # max number of grains/nodes for microstructures in the dataset\n",
    "        features = 5     # number of features of each grain/node\n",
    "\n",
    "        for i in range(1, 493):\n",
    "            # load files\n",
    "            neighbor_file_path = 'data/structure-{}/neighbor.txt'.format(i) \n",
    "            feature_file_path = 'data/structure-{}/feature.txt'.format(i)    \n",
    "            property_file_path = 'data/structure-{}/property.txt'.format(i)\n",
    "            neighbor = np.loadtxt(neighbor_file_path)\n",
    "            feature = np.loadtxt(feature_file_path)\n",
    "            proprty = np.loadtxt(property_file_path)\n",
    "\n",
    "            # feature data manipulation\n",
    "            feature = np.delete(feature, 0, axis=1) # remove the first column (Grain ID)\n",
    "            feature[:, [3]] = (feature[:, [3]] - np.mean(feature[:, [3]])) / np.std(feature[:, [3]]) # normalize grain size\n",
    "            feature[:, [4]] = (feature[:, [4]] - np.mean(feature[:, [4]])) / np.std(feature[:, [4]]) # normalize number of neighbors\n",
    "\n",
    "            # normalize the adjaciency matrix\n",
    "            np.fill_diagonal(neighbor, 1)  # add the identity matrix\n",
    "            D=np.sum(neighbor,axis=0)    # calculate the diagnoal element of D\n",
    "            D_inv=np.diag(np.power(D,-0.5)) # construct D\n",
    "            neighbor=np.matmul(D_inv, np.matmul(neighbor,D_inv)) # symmetric normalization of adjacency matrix \n",
    "            \n",
    "            # match dimension to the max dimension for neighbors\n",
    "            result = np.zeros((max_node, max_node))\n",
    "            result[:neighbor.shape[0], :neighbor.shape[1]] = neighbor\n",
    "            neighbor = result\n",
    "\n",
    "            # match dimension to the max dimension for features\n",
    "            result = np.zeros((max_node, features))\n",
    "            result[:feature.shape[0], :feature.shape[1]] = feature\n",
    "            feature = result\n",
    "\n",
    "            # convert the feature matrix and adjacency matrix to sparse matrix\n",
    "            feature = sparse.csr_matrix(feature)\n",
    "            neighbor = sparse.csr_matrix(neighbor)\n",
    "\n",
    "            #delete data points with negative properties\n",
    "            proprty = proprty[proprty.min(axis=1)>=0,:]\n",
    "\n",
    "            #get the dimension of proprty\n",
    "            num_properties, width = np.shape(proprty)\n",
    "\n",
    "            # independent variable t, the external field\n",
    "            t = np.delete(proprty, 1, axis=1)\n",
    "\n",
    "            # label, the magnetostriction\n",
    "            label = np.delete(proprty, 0, axis=1)\n",
    "\n",
    "            #change it to the several data points\n",
    "            if num_properties == 0:\n",
    "                multiple_neighbor = []\n",
    "                multiple_feature = []\n",
    "            elif num_properties == 1:\n",
    "                multiple_neighbor = [neighbor]\n",
    "                multiple_feature = [feature]\n",
    "            elif num_properties == 2:\n",
    "                multiple_neighbor = [neighbor, neighbor]\n",
    "                multiple_feature = [feature, feature]\n",
    "            elif num_properties == 3:\n",
    "                multiple_neighbor = [neighbor, neighbor, neighbor]\n",
    "                multiple_feature = [feature, feature, feature]\n",
    "            elif num_properties == 4:\n",
    "                multiple_neighbor = [neighbor, neighbor, neighbor, neighbor]\n",
    "                multiple_feature = [feature, feature, feature,feature]\n",
    "            elif num_properties == 5:\n",
    "                multiple_neighbor = [neighbor, neighbor, neighbor, neighbor, neighbor]\n",
    "                multiple_feature = [feature, feature, feature, feature, feature] \n",
    "                \n",
    "            # concatenating the matrices\n",
    "            if i == 1:\n",
    "                adjacency_matrix = multiple_neighbor\n",
    "                node_attr_matrix = multiple_feature\n",
    "                t_matrix = t\n",
    "                label_matrix = label\n",
    "            else:\n",
    "                adjacency_matrix = np.concatenate((adjacency_matrix, multiple_neighbor))\n",
    "                node_attr_matrix = np.concatenate((node_attr_matrix, multiple_feature))\n",
    "                t_matrix = np.concatenate((t_matrix, t))\n",
    "                label_matrix = np.concatenate((label_matrix, label))\n",
    "\n",
    "        # normalize the independent variable t matrix\n",
    "        t_matrix = t_matrix / 10000\n",
    "\n",
    "        # normalize the label matrix\n",
    "        label_mean = np.mean(label_matrix)\n",
    "        label_std = np.std(label_matrix)\n",
    "        label_matrix = (label_matrix - label_mean) / label_std\n",
    "\n",
    "        # save the mean and standard deviation of label\n",
    "        norm = np.array([label_mean, label_std])\n",
    "        np.savez_compressed('norm.npz', norm=norm)\n",
    "\n",
    "        self.adjacency_matrix = np.array(adjacency_matrix)\n",
    "        self.node_attr_matrix = np.array(node_attr_matrix)\n",
    "        self.t_matrix = np.array(t_matrix)\n",
    "        self.label_matrix = np.array(label_matrix)\n",
    "\n",
    "        print('--------------------')\n",
    "        print('Training Data:')\n",
    "        print('adjacency matrix:\\t', self.adjacency_matrix.shape)\n",
    "        print('node attribute matrix:\\t', self.node_attr_matrix.shape)\n",
    "        print('t matrix:\\t\\t', self.t_matrix.shape)\n",
    "        print('label name:\\t\\t', self.label_matrix.shape)\n",
    "        print('--------------------')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.adjacency_matrix)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        adjacency_matrix = self.adjacency_matrix[idx].todense()\n",
    "        node_attr_matrix = self.node_attr_matrix[idx].todense()\n",
    "        t_matrix = self.t_matrix[idx]\n",
    "        label_matrix = self.label_matrix[idx]\n",
    "\n",
    "        adjacency_matrix = torch.from_numpy(adjacency_matrix)\n",
    "        node_attr_matrix = torch.from_numpy(node_attr_matrix)\n",
    "        t_matrix = torch.from_numpy(t_matrix)\n",
    "        label_matrix = torch.from_numpy(label_matrix)\n",
    "        return adjacency_matrix, node_attr_matrix, t_matrix, label_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V6J2jptPGLR"
   },
   "outputs": [],
   "source": [
    "# functions for dividing the data into different folds\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# split the indices into different folds\n",
    "def split_data(num_folds):\n",
    "    dataset = GraphDataSet()\n",
    "    num_of_data = dataset.__len__()\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "    indices = []\n",
    "    for i, (_, index) in enumerate(kf.split(np.arange(num_of_data))):\n",
    "        np.random.shuffle(index)\n",
    "        indices.append(index)\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "# save the indices\n",
    "def extract_graph_data(out_file_path, indices):\n",
    "    np.savez_compressed(out_file_path, indices = indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCDeWVe5XFiV"
   },
   "outputs": [],
   "source": [
    "# functions for the GNN model\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "def tensor_to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x.float())\n",
    "\n",
    "\n",
    "def variable_to_numpy(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    x = x.data.numpy()\n",
    "    return x\n",
    "\n",
    "\n",
    "# the MPL layer\n",
    "class Message_Passing(nn.Module):\n",
    "    def forward(self, x, adjacency_matrix):\n",
    "        neighbor_nodes = torch.bmm(adjacency_matrix, x)\n",
    "        logging.debug('neighbor message\\t', neighbor_nodes.size())\n",
    "        #x = x + neighbor_nodes\n",
    "        logging.debug('x shape\\t', x.size())\n",
    "        return x\n",
    "\n",
    "# the GNN model\n",
    "class GraphModel(nn.Module):\n",
    "    def __init__(self, max_node_num, atom_attr_dim, latent_dim):\n",
    "        super(GraphModel, self).__init__()\n",
    "\n",
    "        self.max_node_num = max_node_num    # max number of grains/nodes for microstructures in the dataset\n",
    "        self.atom_attr_dim = atom_attr_dim  # number of features of each grain/node before passing MPLs\n",
    "        self.latent_dim = latent_dim        # number of features of each grain/node after passing MPls\n",
    "\n",
    "        # MPLs\n",
    "        self.graph_modules = nn.Sequential(OrderedDict([\n",
    "            ('message_passing_0', Message_Passing()),\n",
    "            ('dense_0', nn.Linear(self.atom_attr_dim, 50)),\n",
    "            ('activation_0', nn.Sigmoid()),\n",
    "            ('message_passing_1', Message_Passing()),\n",
    "            ('dense_1', nn.Linear(50, self.latent_dim)),\n",
    "            ('activation_1', nn.Sigmoid()),\n",
    "        ]))\n",
    "        \n",
    "        # FLs\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(self.max_node_num * self.latent_dim + 1, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        return\n",
    "\n",
    "    def forward(self, node_attr_matrix, adjacency_matrix, t_matrix):\n",
    "        node_attr_matrix = node_attr_matrix.float()\n",
    "        adjacency_matrix = adjacency_matrix.float()\n",
    "        x = node_attr_matrix\n",
    "        logging.debug('shape\\t', x.size())\n",
    "\n",
    "        for (name, module) in self.graph_modules.named_children():\n",
    "            if 'message_passing' in name:\n",
    "                x = module(x, adjacency_matrix=adjacency_matrix)\n",
    "            else:\n",
    "                x = module(x)\n",
    "\n",
    "        # Before flatten, the size should be [Batch size, max_node_num, latent_dim]\n",
    "        logging.debug('size of x after GNN\\t', x.size())\n",
    "        # After flatten is the graph representation\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        logging.debug('size of x after GNN\\t', x.size())\n",
    "\n",
    "        # Concatenate [x, t]\n",
    "        x = torch.cat((x, t_matrix), 1)\n",
    "\n",
    "        x = self.fully_connected(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# model training\n",
    "def train(model, data_loader,epochs,checkpoint_dir,optimizer,criterion,test_dataloader, running_index):\n",
    "    #model.train()\n",
    "\n",
    "    print()\n",
    "    print(\"*** Training started! ***\")\n",
    "    print()\n",
    "\n",
    "    filename=\"Learning_Output_{}.txt\".format(running_index)\n",
    "    output=open(filename, \"w\")\n",
    "    print('Epoch Training_time Training_MSE Testing_MSE',file=output)\n",
    "    output.flush()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # save checkpoints\n",
    "        if epoch % (epochs / 10) == 0 or epoch == epochs-1:\n",
    "            torch.save(model.state_dict(), '{}/checkpoint_{}.pth'.format(checkpoint_dir, epoch))\n",
    "\n",
    "        train_start_time = time.time()\n",
    "\n",
    "        for batch_id, (adjacency_matrix, node_attr_matrix, t_matrix, label_matrix) in enumerate(data_loader):\n",
    "            adjacency_matrix = tensor_to_variable(adjacency_matrix)\n",
    "            node_attr_matrix = tensor_to_variable(node_attr_matrix)\n",
    "            t_matrix = tensor_to_variable(t_matrix)\n",
    "            label_matrix = tensor_to_variable(label_matrix)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(adjacency_matrix=adjacency_matrix, node_attr_matrix=node_attr_matrix, t_matrix=t_matrix) # model prediction\n",
    "            loss = criterion(y_pred, label_matrix) # calculate loss\n",
    "            loss.backward()    # back propagation\n",
    "            optimizer.step()   # update weight\n",
    "\n",
    "        train_end_time = time.time()\n",
    "        _, training_MSE = test(model, data_loader, 'Training',False,criterion,running_index)\n",
    "        _, testing_MSE = test(model, test_dataloader, 'Test', False,criterion,running_index)\n",
    "        print('%d %.3f %e %e' % (epoch, train_end_time-train_start_time, training_MSE, testing_MSE), file=output)\n",
    "        output.flush()\n",
    "    \n",
    "    output.close()\n",
    "\n",
    "def test(model, data_loader, test_or_tr, printcond,criterion,running_index):\n",
    "    model.eval()\n",
    "    if data_loader is None:\n",
    "        return None, None\n",
    "\n",
    "    y_label_list, y_pred_list, total_loss = [], [], 0\n",
    "\n",
    "    for batch_id, (adjacency_matrix, node_attr_matrix, t_matrix, label_matrix) in enumerate(data_loader):\n",
    "        adjacency_matrix = tensor_to_variable(adjacency_matrix)\n",
    "        node_attr_matrix = tensor_to_variable(node_attr_matrix)\n",
    "        t_matrix = tensor_to_variable(t_matrix)\n",
    "        label_matrix = tensor_to_variable(label_matrix)\n",
    "\n",
    "        y_pred = model(adjacency_matrix=adjacency_matrix, node_attr_matrix=node_attr_matrix, t_matrix=t_matrix)\n",
    "\n",
    "        y_label_list.extend(variable_to_numpy(label_matrix))\n",
    "        y_pred_list.extend(variable_to_numpy(y_pred))\n",
    "\n",
    "    norm = np.load('norm.npz', allow_pickle=True)['norm']\n",
    "    label_mean, label_std = norm[0], norm[1]\n",
    "\n",
    "    # get the original value of true value and predicted value\n",
    "    y_label_list = np.array(y_label_list) * label_std + label_mean\n",
    "    y_pred_list = np.array(y_pred_list) * label_std + label_mean\n",
    "\n",
    "    # calculate the MARE and MSE \n",
    "    total_MARE = macro_avg_err(y_pred_list, y_label_list)\n",
    "    total_mse = criterion(torch.from_numpy(y_pred_list), torch.from_numpy(y_label_list)).item()\n",
    "\n",
    "    length, w = np.shape(y_label_list)\n",
    "    if printcond:\n",
    "        filename=\"{}_Output_{}.txt\".format(test_or_tr, running_index)\n",
    "        output=open(filename, \"w\")\n",
    "        print('{} Set Predictions: '.format(test_or_tr), file=output)\n",
    "        output.flush()\n",
    "        print('True_Value Predicted_value',file=output)\n",
    "        output.flush()\n",
    "        for i in range(0, length):\n",
    "            print('%f, %f' % (y_label_list[i], y_pred_list[i]),file=output)\n",
    "            output.flush()\n",
    "\n",
    "    return total_MARE, total_mse\n",
    "\n",
    "def get_data(batch_size,idx_path,running_index,folds):\n",
    "    indices = np.load(idx_path, allow_pickle=True)['indices']\n",
    "    test_idx = indices[running_index]\n",
    "    train_idx = indices[[i for i in range(folds) if i != running_index]]\n",
    "    train_idx = [item for sublist in train_idx for item in sublist]\n",
    "\n",
    "    dataset = GraphDataSet()\n",
    "    train_data = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                   sampler=SubsetRandomSampler(train_idx))\n",
    "    test_data = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                  sampler=SubsetRandomSampler(test_idx))\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def macro_avg_err(Y_prime, Y):\n",
    "    if type(Y_prime) is np.ndarray:\n",
    "        return np.sum(np.abs(Y - Y_prime)) / np.sum(np.abs(Y))\n",
    "    return torch.sum(torch.abs(Y-Y_prime)) / torch.sum(torch.abs(Y))\n",
    "\n",
    "def GNNmodel(max_node_num=300, atom_attr_dim=5, latent_dim=5, epochs=1000, batch_size=32, learning_rate=1e-4, min_learning_rate=1e-5, seed=123, checkpoint_dir='checkpoints/',running_index=0, folds=10, idx_path='indices.npz'):\n",
    "    \n",
    "    # make the directory of checkpoint\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Define the model\n",
    "    model = GraphModel(max_node_num=max_node_num, atom_attr_dim=atom_attr_dim, latent_dim=latent_dim)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # optimizer\n",
    "    criterion = nn.MSELoss()  # loss\n",
    "\n",
    "    # get the data\n",
    "    train_dataloader, test_dataloader = get_data(batch_size=batch_size,idx_path=idx_path,running_index=running_index,folds=folds)\n",
    "\n",
    "    # train the mode\n",
    "    train(model=model, data_loader=train_dataloader,epochs=epochs,checkpoint_dir=checkpoint_dir,optimizer=optimizer,criterion=criterion,test_dataloader=test_dataloader,running_index=running_index)\n",
    "    # predictions on the entire training and test datasets\n",
    "    train_rel, train_mse = test(model=model, data_loader=train_dataloader, test_or_tr='Training',printcond=True,criterion=criterion,running_index=running_index)\n",
    "    test_rel, test_mse = test(model=model, data_loader=test_dataloader, test_or_tr='Test', printcond=True,criterion=criterion, running_index=running_index)\n",
    "\n",
    "    print()\n",
    "    print('--------------------')\n",
    "    print()\n",
    "    print(\"Running index: {}\".format(running_index))\n",
    "    print(\"Training Relative Error: {:.3f}%\".format(100 * train_rel))\n",
    "    print(\"Test Relative Error: {:.3f}%\".format(100 * test_rel))\n",
    "    print(\"Training MSE: {}\".format(train_mse))\n",
    "    print(\"Test MSE: {}\".format(test_mse))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0sy4VRy-C7v"
   },
   "outputs": [],
   "source": [
    "# functions for interpretation\n",
    "import math\n",
    "def tensor_to_variable_grad(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x.float(),requires_grad=True)\n",
    "\n",
    "def variable_to_numpy_grad(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    x = x.data.numpy()\n",
    "    return x\n",
    "\n",
    "# calculate the gradient of points\n",
    "def gradient_calculation(adjacency_matrix, node_attr_matrix, t_matrix, model):\n",
    "    adjacency_matrix=tensor_to_variable_grad(adjacency_matrix)\n",
    "    node_attr_matrix=tensor_to_variable_grad(node_attr_matrix)\n",
    "    t_matrix=tensor_to_variable_grad(t_matrix)\n",
    "    \n",
    "    label=model(adjacency_matrix=adjacency_matrix, node_attr_matrix=node_attr_matrix, t_matrix=t_matrix)\n",
    "    label.backward()  # back propagation\n",
    "    grad_node_attr_matrix=variable_to_numpy_grad(node_attr_matrix.grad) # get the gradient of features\n",
    "    grad_t_matrix=variable_to_numpy_grad(t_matrix.grad)  # get the gradient of external field\n",
    "    \n",
    "    return grad_node_attr_matrix, grad_t_matrix\n",
    "    \n",
    "def Intergrated_gradient_calculation(adajacency_matrix, node_attr_matrix, t_matrix, model,steps=200):\n",
    "    nsize=np.asarray(node_attr_matrix.size())\n",
    "    tsize=np.asarray(t_matrix.size())\n",
    "    baseline_node_attr_matrix=torch.zeros((1,nsize[0],nsize[1]))\n",
    "    baseline_t_matrix=torch.zeros((1,tsize[0]))\n",
    "\n",
    "    #specify baseline: same with input graph except the Euler angles\n",
    "    alpha = 0.5*math.pi\n",
    "    beta = 0.5*math.pi\n",
    "    gamma = 0.5*math.pi\n",
    "    for i in range(nsize[0]):\n",
    "        if node_attr_matrix[i][0] != 0:\n",
    "            baseline_node_attr_matrix[0][i][0]=alpha\n",
    "        if node_attr_matrix[i][1] != 0:\n",
    "            baseline_node_attr_matrix[0][i][1]=beta\n",
    "        if node_attr_matrix[i][2] != 0:\n",
    "            baseline_node_attr_matrix[0][i][2]=gamma\n",
    "        baseline_node_attr_matrix[0][i][3]=node_attr_matrix[i][3]\n",
    "        baseline_node_attr_matrix[0][i][4]=node_attr_matrix[i][4]\n",
    "\n",
    "    for i in range(tsize[0]):\n",
    "        baseline_t_matrix[0][i]=t_matrix[i]\n",
    "        \n",
    "    adajacency_matrix=torch.reshape(adajacency_matrix,(1,nsize[0],nsize[0]))\n",
    "\n",
    "    grad_node_attr_matrix=np.zeros((1,nsize[0],nsize[1]))\n",
    "    grad_t_matrix=np.zeros((1,tsize[0]))\n",
    "    \n",
    "    for step in range(steps):\n",
    "        temp_node_attr_matrix=np.zeros((1,nsize[0],nsize[1]))\n",
    "        temp_t_matrix=np.zeros((1,tsize[0]))\n",
    "        temp_node_attr_matrix=torch.from_numpy(temp_node_attr_matrix)\n",
    "        temp_t_matrix=torch.from_numpy(temp_t_matrix)\n",
    "        \n",
    "        for i in range(nsize[0]):\n",
    "            for j in range(nsize[1]):\n",
    "                temp_node_attr_matrix[0][i][j]=baseline_node_attr_matrix[0][i][j]+(node_attr_matrix[i][j]-baseline_node_attr_matrix[0][i][j])*step/steps\n",
    "                \n",
    "        for i in range(tsize[0]):\n",
    "            temp_t_matrix[0][i]=baseline_t_matrix[0][i]+(t_matrix[i]-baseline_t_matrix[0][i])*step/steps\n",
    "\n",
    "\n",
    "        temp_grad_node_attr_matrix, temp_grad_t_matrix=gradient_calculation(adajacency_matrix, temp_node_attr_matrix, temp_t_matrix, model)\n",
    "        grad_node_attr_matrix=grad_node_attr_matrix+temp_grad_node_attr_matrix\n",
    "        grad_t_matrix=grad_t_matrix+temp_grad_t_matrix\n",
    "\n",
    "    node_attr_matrix=node_attr_matrix.numpy()\n",
    "    baseline_node_attr_matrix=baseline_node_attr_matrix.numpy()\n",
    "    t_matrix=t_matrix.numpy()\n",
    "    baseline_t_matrix=baseline_t_matrix.numpy()\n",
    "        \n",
    "    for i in range(nsize[0]):\n",
    "            for j in range(nsize[1]):\n",
    "                grad_node_attr_matrix[0][i][j]=(node_attr_matrix[i][j]-baseline_node_attr_matrix[0][i][j])*grad_node_attr_matrix[0][i][j]/steps\n",
    "                \n",
    "    for i in range(tsize[0]):\n",
    "        grad_t_matrix[0][i]=(t_matrix[i]-baseline_t_matrix[0][i])*grad_t_matrix[0][i]/steps\n",
    "\n",
    "    return grad_node_attr_matrix, grad_t_matrix    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "bx3hAT9bPwKX",
    "outputId": "38720ad8-b985-441e-c976-edd4d75f4a2f"
   },
   "outputs": [],
   "source": [
    "# save and output indices\n",
    "num_folds=10\n",
    "out_file_path='indices.npz'\n",
    "print(\"Output File Path: {}\".format(out_file_path))\n",
    "\n",
    "indices = split_data(num_folds=num_folds)\n",
    "extract_graph_data(out_file_path=out_file_path, indices = indices)\n",
    "\n",
    "print(\"Data successfully split into {} folds!\".format(num_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4TwKgTGHYOWG",
    "outputId": "b1a2207f-1de7-47b4-85bb-54d9a11504ad"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model=GNNmodel(latent_dim=5,epochs=1000,batch_size=32,learning_rate=1e-4,min_learning_rate=1e-5,running_index=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CClEPBMkyLSx"
   },
   "outputs": [],
   "source": [
    "# Reload the data for getting interpretation results\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import sparse\n",
    "\n",
    "class GraphDataSet_interpretable(Dataset):\n",
    "    def __init__(self):\n",
    "        max_node = 300\n",
    "        features = 5\n",
    "\n",
    "        for i in range(1, 493):\n",
    "            # load files\n",
    "            neighbor_file_path = 'data/structure-{}/neighbor.txt'.format(i)\n",
    "            feature_file_path = 'data/structure-{}/feature.txt'.format(i)\n",
    "            property_file_path = 'data/structure-{}/property.txt'.format(i)\n",
    "            neighbor = np.loadtxt(neighbor_file_path)\n",
    "            feature = np.loadtxt(feature_file_path)\n",
    "            proprty = np.loadtxt(property_file_path)\n",
    "\n",
    "            # feature data manipulation\n",
    "            feature = np.delete(feature, 0, axis=1)\n",
    "            feature[:, [3]] = (feature[:, [3]] - np.mean(feature[:, [3]])) / np.std(feature[:, [3]])\n",
    "            feature[:, [4]] = (feature[:, [4]] - np.mean(feature[:, [4]])) / np.std(feature[:, [4]])\n",
    "\n",
    "            # normalize the adjaciency matrix\n",
    "            np.fill_diagonal(neighbor, 1)\n",
    "            D=np.sum(neighbor,axis=0)\n",
    "            D_inv=np.diag(np.power(D,-0.5))\n",
    "            neighbor=np.matmul(D_inv, np.matmul(neighbor,D_inv))\n",
    "            \n",
    "            # match dimension to the max dimension for neighbors\n",
    "            result = np.zeros((max_node, max_node))\n",
    "            result[:neighbor.shape[0], :neighbor.shape[1]] = neighbor\n",
    "            neighbor = result\n",
    "\n",
    "            # match dimension to the max dimension for features\n",
    "            result = np.zeros((max_node, features))\n",
    "            result[:feature.shape[0], :feature.shape[1]] = feature\n",
    "            feature = result\n",
    "\n",
    "            feature = sparse.csr_matrix(feature)\n",
    "            neighbor = sparse.csr_matrix(neighbor)\n",
    "\n",
    "            #delete negative value of proprty\n",
    "            proprty = proprty[proprty.min(axis=1)>=0,:]\n",
    "\n",
    "            #get the dimension of proprty\n",
    "            num_properties, width = np.shape(proprty)\n",
    "\n",
    "            # independent variable t\n",
    "            t = np.delete(proprty, 1, axis=1)\n",
    "            t = t[num_properties-1,:]   # get the field of the last data point associated with this microstructure\n",
    "            t = t.reshape((1,1))\n",
    "\n",
    "            # label\n",
    "            label = np.delete(proprty, 0, axis=1)\n",
    "            label = label[num_properties-1,:]  # get the target of the last data point associated with this microstructure\n",
    "            label = label.reshape((1,1))\n",
    "\n",
    "            # gradient will not change for different field value. So only using one point for interpretation\n",
    "            multiple_neighbor = [neighbor]\n",
    "            multiple_feature = [feature]\n",
    "\n",
    "            if i == 1:\n",
    "                adjacency_matrix = multiple_neighbor\n",
    "                node_attr_matrix = multiple_feature\n",
    "                t_matrix = t\n",
    "                label_matrix = label\n",
    "            else:\n",
    "                adjacency_matrix = np.concatenate((adjacency_matrix, multiple_neighbor))\n",
    "                node_attr_matrix = np.concatenate((node_attr_matrix, multiple_feature))\n",
    "                t_matrix = np.concatenate((t_matrix, t))\n",
    "                label_matrix = np.concatenate((label_matrix, label))\n",
    "\n",
    "        # normalize the independent variable t matrix\n",
    "        t_matrix = t_matrix / 10000\n",
    "\n",
    "        # normalize the label matrix\n",
    "        label_mean = np.mean(label_matrix)\n",
    "        label_std = np.std(label_matrix)\n",
    "        label_matrix = (label_matrix - label_mean) / label_std\n",
    "\n",
    "        norm = np.array([label_mean, label_std])\n",
    "        np.savez_compressed('Interpretable_norm.npz', norm=norm)\n",
    "\n",
    "        self.adjacency_matrix = np.array(adjacency_matrix)\n",
    "        self.node_attr_matrix = np.array(node_attr_matrix)\n",
    "        self.t_matrix = np.array(t_matrix)\n",
    "        self.label_matrix = np.array(label_matrix)\n",
    "\n",
    "        print('--------------------')\n",
    "        print('Training Data:')\n",
    "        print('adjacency matrix:\\t', self.adjacency_matrix.shape)\n",
    "        print('node attribute matrix:\\t', self.node_attr_matrix.shape)\n",
    "        print('t matrix:\\t\\t', self.t_matrix.shape)\n",
    "        print('label name:\\t\\t', self.label_matrix.shape)\n",
    "        print('--------------------')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.adjacency_matrix)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        adjacency_matrix = self.adjacency_matrix[idx].todense()\n",
    "        node_attr_matrix = self.node_attr_matrix[idx].todense()\n",
    "        t_matrix = self.t_matrix[idx]\n",
    "        label_matrix = self.label_matrix[idx]\n",
    "\n",
    "        adjacency_matrix = torch.from_numpy(adjacency_matrix)\n",
    "        node_attr_matrix = torch.from_numpy(node_attr_matrix)\n",
    "        t_matrix = torch.from_numpy(t_matrix)\n",
    "        label_matrix = torch.from_numpy(label_matrix)\n",
    "        return adjacency_matrix, node_attr_matrix, t_matrix, label_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "852ZWn5o1Djv",
    "outputId": "b50d7576-70db-450b-cc84-e7d0b4807084"
   },
   "outputs": [],
   "source": [
    "dataset=GraphDataSet_interpretable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zScAh0Nib9vX"
   },
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "i=100\n",
    "Graph=dataset[i]\n",
    "adajacency_matrix=Graph[0]\n",
    "node_attr_matrix=Graph[1]\n",
    "t_matrix=Graph[2]\n",
    "grad_node_attr_matrix, grad_t_matrix=Intergrated_gradient_calculation(adajacency_matrix, node_attr_matrix, t_matrix, model,steps=200)\n",
    "feature_gradient=grad_node_attr_matrix[0]\n",
    "outputnumber=i+1\n",
    "savetxt(\"feature_grad_{0}.csv\".format(outputnumber), feature_gradient, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GNN0602_interpretation1011_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
